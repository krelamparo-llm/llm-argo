# Anthropic Best Practices - Implementation Plan

## Executive Summary

Based on comprehensive research of Anthropic's 2024-2025 engineering blog and official documentation, this document outlines improvements to Argo Brain's agent architecture. The recommendations are prioritized by impact and implementation effort.

**Research Sources**:
- [Building Effective AI Agents](https://www.anthropic.com/research/building-effective-agents)
- [Writing Tools for Agents](https://www.anthropic.com/engineering/writing-tools-for-agents)
- [Effective Context Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)
- [Multi-Agent Research System](https://www.anthropic.com/engineering/multi-agent-research-system)
- [Long-Running Harnesses](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents)
- [Advanced Tool Use](https://www.anthropic.com/engineering/advanced-tool-use)

---

## Current State Analysis

### âœ… What We're Doing Right

1. **Agent Loop Pattern** - Our architecture matches Anthropic's recommended "agent" pattern (LLM dynamically directing tool usage in a loop)
2. **XML Structure Tags** - We use `<research_plan>`, `<synthesis>`, `<think>`, `<confidence>`, `<gaps>`
3. **MAX_TOOL_CALLS Limit** - Prevents runaway loops (25 iterations)
4. **Tool Policy System** - Guardrails for safety and approval/rejection
5. **Planning-First Architecture** - Research mode starts with explicit planning phase
6. **Explicit Synthesis Trigger** - Just implemented! Matches Anthropic's coordination patterns

### âŒ Current Issues

1. **Context Overflow** - Hitting 4330 tokens with ctx=4096, experiencing "context rot"
2. **Sequential Tool Execution** - Tools run one-by-one instead of in parallel (slow)
3. **Full Text Loading** - web_access always returns 10K+ token articles
4. **Tool Description Clarity** - Could be more explicit per Anthropic guidance
5. **No Context Compaction** - Long conversations consume excessive tokens

---

## Implementation Phases

---

## ðŸ”¥ PHASE 1: Immediate Wins (2-3 hours)

**Goal**: Quick, high-impact improvements with minimal risk

### 1.1 Improve Tool Descriptions â­â­â­

**Source**: [Writing Tools for Agents](https://www.anthropic.com/engineering/writing-tools-for-agents)

**Anthropic's Guidance**:
> "Even small refinements to tool descriptions can yield dramatic improvements... Describe tools as you would to a new team member, making implicit context explicit."

**Current State**: Basic descriptions without examples or edge cases

**Changes Required**:

#### A. web_search_tool.py
```python
# BEFORE
description = "Search the web for current information"

# AFTER
description = """Search the web for current information using DuckDuckGo.

**When to use**:
- Finding recent news, articles, or documentation
- Discovering authoritative sources on a topic
- Getting multiple perspectives on a question

**Parameters**:
- query (str): Natural language search query, 3-10 words recommended
  Example: "machine learning best practices 2025"
  Example: "Python asyncio tutorial"

**Returns**: List of search results with titles, URLs, and snippets

**Best practices**:
- Use specific, focused queries rather than broad terms
- Include year for time-sensitive topics (e.g., "React hooks 2025")
- Avoid overly long queries (>15 words become less effective)

**Edge cases**:
- Very recent events may have limited results
- Highly technical terms work better than colloquial language
"""
```

#### B. web_access_tool.py
```python
# BEFORE
description = "Fetch and read content from a URL"

# AFTER
description = """Fetch and read the full content from a specific URL.

**When to use**:
- After web_search identifies a promising source
- When you need the complete article/page content
- To verify specific claims or details from search snippets

**Parameters**:
- url (str): Valid HTTP/HTTPS URL to fetch
  Example: "https://docs.python.org/3/library/asyncio.html"
- response_format (str, optional): "concise" or "detailed" (default: "concise")
  - "concise": Returns 200-word summary + key facts (faster, fewer tokens)
  - "detailed": Returns full article text (use for deep analysis)

**Returns**:
- Concise mode: Title, URL, summary, key facts, metadata
- Detailed mode: Full article text with metadata

**Best practices**:
- Use "concise" mode first to evaluate relevance
- Only use "detailed" mode when you need to cite specific passages
- Prefer official documentation and primary sources

**Edge cases**:
- Paywalled content may return partial text
- Dynamic JavaScript sites may have limited content
- PDF links will attempt text extraction
"""
```

#### C. memory_query_tool.py
```python
# AFTER
description = """Query Argo's long-term memory for previously stored information.

**When to use**:
- User asks about past conversations or research
- Looking for information you previously ingested
- Checking if a topic has been covered before

**Parameters**:
- query (str): Natural language query describing what to find
  Example: "machine learning projects from last month"
  Example: "research on Python performance"

**Returns**: Relevant memory chunks with timestamps and sources

**Best practices**:
- Query memory BEFORE doing web searches for topics you may have researched before
- Use temporal terms when relevant ("last week", "recent")
- Combine with web_search for updated information

**Edge cases**:
- Empty results mean topic hasn't been stored yet
- Queries must match semantic meaning, not exact phrasing
"""
```

#### D. memory_write_tool.py
```python
# AFTER
description = """Store important information in Argo's long-term memory.

**When to use**:
- After completing substantial research
- User explicitly asks to remember something
- Storing structured notes for future retrieval

**Parameters**:
- content (str): The information to store (plain text or markdown)
- metadata (dict, optional): Additional context (tags, source, date)

**Returns**: Confirmation of storage

**Best practices**:
- Write clear, self-contained summaries (not raw tool outputs)
- Include source URLs in content when relevant
- Use markdown formatting for structure

**Edge cases**:
- Duplicate content is deduplicated automatically
- Very large content may be chunked
"""
```

**Files to Modify**:
- `argo_brain/tools/web_search_tool.py`
- `argo_brain/tools/web_access_tool.py`
- `argo_brain/tools/memory_query_tool.py`
- `argo_brain/tools/memory_write_tool.py`

**Expected Impact**: 10-25% better tool selection accuracy

**Time Estimate**: 30 minutes

---

### 1.2 Add response_format Parameter to web_access â­â­â­

**Source**: [Writing Tools for Agents](https://www.anthropic.com/engineering/writing-tools-for-agents)

**Anthropic's Guidance**:
> "Implement a `response_format` enum parameter allowing agents to choose between 'concise' and 'detailed' responses. Detailed responses enable downstream tool calls requiring IDs; concise responses conserve tokens."

**Current Issue**: web_access returns full article text (often 10K+ tokens), causing context bloat

**Solution**: Two-tier response system

**Implementation**:

#### A. Update ToolRequest metadata handling
```python
# In web_access_tool.py run() method

def run(self, request: ToolRequest) -> ToolResult:
    url = request.metadata.get("url")
    response_format = request.metadata.get("response_format", "concise")

    # Fetch content (existing logic)
    content = self._fetch_url(url)

    if response_format == "concise":
        # Generate 200-word summary
        summary = self._generate_summary(content, max_words=200)
        key_facts = self._extract_key_facts(content, max_facts=5)

        return ToolResult(
            content=f"**Summary**: {summary}\n\n**Key Facts**:\n{key_facts}",
            metadata={
                "url": url,
                "format": "concise",
                "full_length": len(content),
                "word_count": len(content.split())
            }
        )
    else:  # detailed
        return ToolResult(
            content=content,
            metadata={
                "url": url,
                "format": "detailed",
                "word_count": len(content.split())
            }
        )

def _generate_summary(self, content: str, max_words: int = 200) -> str:
    """Generate concise summary using LLM or extractive summarization."""
    # Option 1: Use LLM for abstractive summary
    # Option 2: Extract first N sentences + key paragraphs
    # For now, implement simple extractive approach
    sentences = content.split('. ')
    summary_sentences = sentences[:5]  # First 5 sentences
    summary = '. '.join(summary_sentences) + '.'
    return summary[:max_words * 6]  # Rough word limit

def _extract_key_facts(self, content: str, max_facts: int = 5) -> str:
    """Extract key facts as bullet points."""
    # Simple implementation: extract numbered/bulleted lists
    # Or: Extract sentences with key phrases (numbers, dates, names)
    # For now: First paragraph + any lists
    paragraphs = content.split('\n\n')
    facts = []
    for para in paragraphs[:3]:
        if 'â€¢' in para or '-' in para or any(c.isdigit() for c in para):
            facts.append(para.strip())
    return '\n'.join(f"â€¢ {fact}" for fact in facts[:max_facts])
```

#### B. Update tool description (already covered in 1.1)

**Files to Modify**:
- `argo_brain/tools/web_access_tool.py`

**Expected Impact**:
- 80% token reduction per web_access call (10K â†’ 2K tokens typical)
- Faster research (less context to process)
- Option to get full text when needed

**Time Estimate**: 1 hour

---

### 1.3 Parallel Tool Execution â­â­â­

**Source**: [Multi-Agent Research System](https://www.anthropic.com/engineering/multi-agent-research-system)

**Anthropic's Pattern**:
> "Individual agents use 3+ parallel tool callsâ€”reducing research time by up to 90%"

**Current Issue**: Tools execute sequentially (web_search #1, then #2, then #3...)

**Solution**: ThreadPoolExecutor for parallel execution

**Implementation**:

```python
# In orchestrator.py, modify the tool execution loop

from concurrent.futures import ThreadPoolExecutor, as_completed

# Around line 470 in send_message()
plan_payload = self._maybe_parse_plan(response_text)
if plan_payload:
    proposals = plan_payload["proposals"]
    approved, rejections = self.tool_policy.review(proposals, self.tool_registry)

    if rejections:
        msg = json.dumps({"rejected": rejections}, ensure_ascii=False)
        extra_messages.append(ChatMessage(role="system", content=f"POLICY_REJECTION {msg}"))

    if approved:
        # NEW: Execute approved tools in parallel
        if len(approved) > 1:
            self.logger.info(f"Executing {len(approved)} tools in parallel", extra={"session_id": session_id})
            results = self._execute_tools_parallel(approved, session_id, user_message, active_mode)
        else:
            # Single tool - execute normally
            results = [self._execute_single_tool(approved[0], session_id, user_message, active_mode)]

        # Process results
        for proposal, result in zip(approved, results):
            iterations += 1
            if iterations >= self.MAX_TOOL_CALLS:
                break

            tool_results_accum.append(result)

            # Track research progress
            research_stats["tool_calls"] += 1
            if proposal.tool == "web_search":
                research_stats["searches"] += 1
                query = proposal.arguments.get("query", user_message)
                research_stats["search_queries"].append(str(query))
            elif proposal.tool == "web_access" and result.metadata:
                url = result.metadata.get("url")
                if url:
                    research_stats["unique_urls"].add(url)
                    research_stats["sources_fetched"] += 1

        # Update context and add messages
        context = self.memory_manager.get_context_for_prompt(
            session_id,
            user_message,
            tool_results=tool_results_accum,
        )

        for proposal in approved:
            arguments = proposal.arguments or {}
            call_json = json.dumps({"tool_name": proposal.tool, "arguments": arguments}, ensure_ascii=False)
            extra_messages.append(ChatMessage(role="assistant", content=f"TOOL_CALL {call_json}"))

        for result in results:
            result_msg = self._format_tool_result_for_prompt(result)
            if active_mode == SessionMode.RESEARCH:
                result_msg += self._format_research_progress(research_stats)
            extra_messages.append(ChatMessage(role="system", content=result_msg))

        continue

def _execute_single_tool(
    self,
    proposal: ProposedToolCall,
    session_id: str,
    user_message: str,
    active_mode: SessionMode
) -> ToolResult:
    """Execute a single tool call."""
    arguments = proposal.arguments or {}
    query_arg = (
        str(arguments.get("query"))
        if arguments.get("query") is not None
        else arguments.get("url")
    )
    query_value = query_arg or user_message

    self.logger.info(
        "Executing tool",
        extra={
            "session_id": session_id,
            "tool": proposal.tool,
            "args_keys": sorted(arguments.keys()),
        },
    )

    return self.run_tool(
        proposal.tool,
        session_id,
        str(query_value),
        metadata=arguments,
        session_mode=active_mode,
    )

def _execute_tools_parallel(
    self,
    proposals: List[ProposedToolCall],
    session_id: str,
    user_message: str,
    active_mode: SessionMode,
    max_workers: int = 3
) -> List[ToolResult]:
    """Execute multiple tools in parallel using ThreadPoolExecutor."""
    results = [None] * len(proposals)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_index = {
            executor.submit(
                self._execute_single_tool,
                proposal,
                session_id,
                user_message,
                active_mode
            ): i
            for i, proposal in enumerate(proposals)
        }

        # Collect results as they complete
        for future in as_completed(future_to_index):
            index = future_to_index[future]
            try:
                results[index] = future.result()
            except Exception as exc:
                self.logger.error(
                    f"Tool execution failed",
                    extra={
                        "session_id": session_id,
                        "tool": proposals[index].tool,
                        "error": str(exc)
                    }
                )
                # Return error result
                results[index] = ToolResult(
                    error=f"Tool execution failed: {exc}",
                    metadata={"tool": proposals[index].tool}
                )

    return results
```

**Files to Modify**:
- `argo_brain/assistant/orchestrator.py`

**Expected Impact**:
- 50-70% faster research queries
- Multiple web_access calls complete simultaneously
- Better user experience (faster results)

**Time Estimate**: 1.5 hours

---

## PHASE 1 SUMMARY

**Total Time**: ~3 hours
**Expected Improvements**:
- Tool selection accuracy: +10-25%
- Context usage: -80% per web_access call
- Research speed: +50-70% faster
- Combined token reduction: ~60-70% in typical research queries

---

## ðŸŸ¢ PHASE 2: Context Optimization (3-4 hours)

**Goal**: Prevent context overflow and improve long-conversation handling

### 2.1 Just-In-Time Context Retrieval â­â­â­

**Source**: [Effective Context Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)

**Anthropic's Pattern**:
> "Rather than pre-loading all data, agents maintain lightweight identifiers (file paths, queries, links) and retrieve data dynamically using tools."

**Current Issue**: We load all RAG chunks upfront, consuming massive tokens

**Solution**: Return summaries/identifiers, create retrieve_context tool for full text

**Implementation**:

#### A. Modify MemoryManager to return identifiers
```python
# In memory_manager.py

def get_context_identifiers(
    self,
    session_id: str,
    query: str,
    max_identifiers: int = 10
) -> List[Dict[str, Any]]:
    """Return lightweight identifiers instead of full content."""
    chunks = self._retrieve_chunks(query, limit=max_identifiers)

    identifiers = []
    for chunk in chunks:
        identifiers.append({
            "id": chunk.id,
            "title": chunk.metadata.get("title", "Untitled"),
            "url": chunk.metadata.get("url"),
            "timestamp": chunk.metadata.get("timestamp"),
            "snippet": chunk.content[:200] + "...",  # First 200 chars
            "relevance_score": chunk.score
        })

    return identifiers

def retrieve_chunk_by_id(self, chunk_id: str) -> Optional[str]:
    """Retrieve full content for a specific chunk."""
    # Implementation to fetch by ID
    pass
```

#### B. Create RetrieveContextTool
```python
# New file: argo_brain/tools/retrieve_context_tool.py

class RetrieveContextTool(Tool):
    name = "retrieve_context"
    description = """Retrieve full content for a specific memory chunk by ID.

    Use this AFTER memory_query shows relevant identifiers.
    Only retrieve chunks you actually need to cite or analyze in detail.
    """

    def run(self, request: ToolRequest) -> ToolResult:
        chunk_id = request.metadata.get("chunk_id")
        content = self.memory_manager.retrieve_chunk_by_id(chunk_id)
        return ToolResult(content=content)
```

#### C. Update build_prompt to use identifiers
```python
# In orchestrator.py

def build_prompt(self, context, user_message, session_mode):
    # ...
    if context.get("memory_chunks"):
        identifiers = self.memory_manager.get_context_identifiers(...)
        context_str += "\n\n## RELEVANT MEMORY (Identifiers)\n"
        for ident in identifiers:
            context_str += f"- [{ident['title']}] (ID: {ident['id']})\n"
            context_str += f"  Snippet: {ident['snippet']}\n"
            context_str += f"  URL: {ident['url']}\n\n"
        context_str += "Use retrieve_context(chunk_id=ID) to get full content.\n"
```

**Expected Impact**: 50-70% token reduction in context building

**Time Estimate**: 2 hours

---

### 2.2 Conversation Compaction â­â­

**Source**: [Context Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)

**Anthropic's Strategy**:
> "Summarize conversation history and restart with compressed context plus recent files, preserving architectural decisions while discarding redundant outputs."

**Implementation**:

```python
# In orchestrator.py

def _compress_tool_results(self, tool_results: List[ToolResult]) -> str:
    """Compress tool results into concise summary."""
    if len(tool_results) <= 5:
        return ""  # Not worth compressing

    summary = "## TOOL EXECUTION SUMMARY\n\n"

    # Group by tool type
    by_tool = {}
    for result in tool_results:
        tool_name = result.metadata.get("tool", "unknown")
        if tool_name not in by_tool:
            by_tool[tool_name] = []
        by_tool[tool_name].append(result)

    # Summarize each tool type
    for tool_name, results in by_tool.items():
        summary += f"**{tool_name}** ({len(results)} calls):\n"
        if tool_name == "web_search":
            queries = [r.metadata.get("query") for r in results if r.metadata.get("query")]
            summary += f"  Searched: {', '.join(queries[:3])}\n"
        elif tool_name == "web_access":
            urls = [r.metadata.get("url") for r in results if r.metadata.get("url")]
            summary += f"  Fetched {len(urls)} sources\n"
            summary += f"  Key sources: {', '.join(urls[:3])}\n"
        summary += "\n"

    return summary

# In send_message() loop
if research_stats["tool_calls"] > 15:
    compressed = self._compress_tool_results(tool_results_accum)
    if compressed:
        extra_messages = [ChatMessage(role="system", content=compressed)]
        tool_results_accum = tool_results_accum[-3:]  # Keep only last 3
```

**Expected Impact**: Prevent context overflow in long research sessions

**Time Estimate**: 1 hour

---

## PHASE 2 SUMMARY

**Total Time**: ~3 hours
**Expected Improvements**:
- Context usage: -60% in typical prompts
- Long conversations: No more overflow errors
- Memory efficiency: Can handle 20+ tool calls

---

## ðŸŸ¡ PHASE 3: Quality Improvements (2-3 hours)

**Goal**: Improve synthesis quality and tool usage patterns

### 3.1 Extended Thinking for Synthesis â­â­

**Source**: [Multi-Agent Research](https://www.anthropic.com/engineering/multi-agent-research-system)

**Pattern**: Use Claude's extended thinking for higher quality synthesis

```python
# In orchestrator.py, when triggering synthesis

if not research_stats.get("synthesis_triggered"):
    research_stats["synthesis_triggered"] = True

    extra_messages.append(ChatMessage(role="system", content=synthesis_prompt))
    self.logger.info("Triggering synthesis phase with extended thinking")

    # Override for synthesis call
    synthesis_response = self.llm_client.chat(
        prompt_messages + extra_messages,
        max_tokens=4096,
        temperature=0.7,  # Higher for creative synthesis
        thinking={"enabled": True, "budget_tokens": 2000}  # NEW
    )
    # ... handle response
```

**Time Estimate**: 30 minutes

---

### 3.2 Dynamic Tool Availability â­

**Source**: [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents)

**Pattern**: Control which tools are available per phase

```python
# In build_tool_manifest()

def build_tool_manifest(self, available_tools: Optional[List[str]] = None) -> str:
    """Build manifest with optional tool filtering."""
    tools = self.tool_registry.list_tools()

    if available_tools:
        tools = [t for t in tools if t.name in available_tools]

    # ... rest of manifest building

# In send_message(), vary tools by phase
if active_mode == SessionMode.RESEARCH:
    if not research_stats["has_plan"]:
        # Planning phase: no tools needed
        available_tools = []
    elif research_stats["tool_calls"] < 10:
        # Exploration phase: search and access only
        available_tools = ["web_search", "web_access"]
    else:
        # Synthesis phase: add memory tools
        available_tools = ["memory_write", "memory_query"]
```

**Time Estimate**: 1 hour

---

## ðŸ“Š Expected Performance Improvements

| Phase | Optimization | Token Reduction | Speed Improvement | Accuracy Gain |
|-------|-------------|----------------|-------------------|---------------|
| **Phase 1** | Tool descriptions | - | - | +10-25% |
| **Phase 1** | Response formats | -80% per call | - | - |
| **Phase 1** | Parallel execution | - | +50-70% | - |
| **Phase 2** | Just-in-time context | -50-70% | - | Prevents context rot |
| **Phase 2** | Compaction | -40% | - | Handles long sessions |
| **Phase 3** | Extended thinking | - | - | +15-30% |

**Combined Expected Impact**:
- **Context usage**: 60-75% reduction (4330 â†’ ~1200 tokens)
- **Research speed**: 50-70% faster (60s â†’ 20-30s per query)
- **Quality**: 20-40% improvement in accuracy and completeness

---

## Implementation Checklist

### Phase 1: Immediate Wins âœ…
- [ ] Update web_search_tool.py description
- [ ] Update web_access_tool.py description
- [ ] Update memory_query_tool.py description
- [ ] Update memory_write_tool.py description
- [ ] Implement response_format in web_access_tool.py
- [ ] Implement _generate_summary() method
- [ ] Implement _extract_key_facts() method
- [ ] Implement parallel tool execution in orchestrator.py
- [ ] Add _execute_single_tool() method
- [ ] Add _execute_tools_parallel() method
- [ ] Test with research query
- [ ] Validate token savings
- [ ] Validate speed improvements

### Phase 2: Context Optimization
- [ ] Implement get_context_identifiers() in memory_manager.py
- [ ] Implement retrieve_chunk_by_id() in memory_manager.py
- [ ] Create retrieve_context_tool.py
- [ ] Update build_prompt() to use identifiers
- [ ] Register retrieve_context tool
- [ ] Implement _compress_tool_results() in orchestrator.py
- [ ] Add compaction logic to send_message() loop
- [ ] Test with long research sessions
- [ ] Validate no context overflow

### Phase 3: Quality Improvements
- [ ] Add extended thinking to synthesis call
- [ ] Implement dynamic tool availability
- [ ] Test synthesis quality improvements
- [ ] A/B test with and without extended thinking

---

## Testing Strategy

### Phase 1 Testing
1. **Baseline measurement**: Run research query, measure tokens and time
2. **After tool descriptions**: Validate improved tool selection
3. **After response_format**: Measure token reduction
4. **After parallel execution**: Measure speed improvement
5. **Compare**: Baseline vs. optimized

### Phase 2 Testing
1. **Context overflow test**: Run 20+ tool call session
2. **Validate**: No context errors, compaction works
3. **Token measurement**: Confirm 50-70% reduction

### Phase 3 Testing
1. **Quality evaluation**: Compare synthesis with/without extended thinking
2. **Human evaluation**: Read outputs for accuracy and completeness

---

## Rollback Plan

Each phase is independent and can be rolled back:

**Phase 1**:
- Tool descriptions: Revert individual files
- response_format: Falls back to full text if not specified
- Parallel execution: Wrapped in try/except, falls back to sequential

**Phase 2**:
- Just-in-time context: Keep old method, add feature flag
- Compaction: Only triggers after threshold, safe to disable

**Phase 3**:
- Extended thinking: Optional parameter, easy to remove
- Dynamic tools: Feature flag controlled

---

## Success Metrics

**Must Have** (Phase 1):
- âœ… Synthesis appears after tool execution
- âœ… No context overflow errors on typical queries
- âœ… Research completes in <60 seconds (from 60-120s)

**Should Have** (Phase 2):
- âœ… Handle 20+ tool calls without overflow
- âœ… Context usage <2000 tokens average
- âœ… Compaction triggers correctly

**Nice to Have** (Phase 3):
- âœ… Extended thinking improves synthesis quality (subjective)
- âœ… Dynamic tools reduce confusion

---

## Future Explorations (Not Prioritized)

### Tool Search Tool
**When**: We have 10+ tools (currently 4)
**Impact**: 85% token reduction with many tools

### Programmatic Tool Calling
**When**: We need complex data processing (aggregates, filtering)
**Impact**: 37% token reduction, eliminates intermediate passes
**Effort**: High (architecture change)

### Multi-Agent Orchestrator
**When**: Research becomes significantly more complex
**Impact**: 90% speed improvement with parallel subagents
**Effort**: Very High (new architecture pattern)

---

## Conclusion

This implementation plan follows Anthropic's proven patterns for building effective agents. Phase 1 provides immediate, measurable improvements with low risk. Phases 2-3 build on this foundation for long-term scalability and quality.

**Recommended Approach**:
1. Implement Phase 1 completely
2. Test and validate improvements
3. Proceed to Phase 2 based on results
4. Phase 3 is optional quality enhancement

**Next Steps**:
1. âœ… Document plan (this file)
2. â†’ Implement Phase 1 (3 hours)
3. â†’ Test and validate
4. â†’ Plan Phase 2 implementation
