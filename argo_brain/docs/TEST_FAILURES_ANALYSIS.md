# Test Failures Analysis & Fixes

**Date**: December 2024
**Test Run**: 11 tests, 7 passed, 4 failed
**Test Results**: `test_results.json`

---

## Executive Summary

| Test | Issue | Root Cause | Severity | Status |
|------|-------|------------|----------|--------|
| TEST-004 | No synthesis or confidence | Research stops at plan + partial execution | HIGH | üîç Investigating |
| TEST-005 | No synthesis, stopped at plan | Research never triggers synthesis (< 3 sources?) | HIGH | üîç Investigating |
| TEST-008 | No clarification request | Model answers from memory instead of asking | MEDIUM | üîç Investigating |
| TEST-009 | Answered wrong topic (Claude) | Security rejection not working for file:// URLs | CRITICAL | üîç Investigating |

---

## A. Test Failures Analysis

### TEST-004: Research Plan Creation (FAILED)
**Input**: "Research the differences between Claude and GPT-4"
**Expected**: Creates plan ‚Üí executes tools ‚Üí gathers 3+ sources ‚Üí provides synthesis with confidence
**Actual**: Created research plan, but **no synthesis or confidence section**

**Output Analysis** (`/tmp/test_test-004_response.txt`):
```xml
<research_plan>
Research question: What are the key differences between Claude and GPT-4 AI models?

Sub-questions:
- What are the architectural differences between Claude and GPT-4?
- How do Claude and GPT-4 perform in reasoning, creativity, and accuracy?
- What are the practical use cases and applications for each model?
- What are the key strengths and weaknesses of each model?

Search strategy:
- "Claude vs GPT-4 architectural differences"
- "Claude vs GPT-4 performance capabilities reasoning creativity accuracy"
- "Claude vs GPT-4 use cases practical applications"
- "Claude 4 vs GPT-4 architecture and capabilities"

Success criteria:
- Comprehensive understanding of architectural, performance, and application differences
- Clear identification of strengths and weaknesses for each model
- Credible sources with specific details about both models

Expected sources:
- Technical documentation from Anthropic (Claude)
- Technical documentation from OpenAI (GPT-4)
- Independent analysis blogs with comparative studies
- Industry expert opinions and benchmarks
</research_plan>
```

**Issue**: Output shows **only the plan**, no tool execution or synthesis followed.

**Hypothesis**:
1. Research mode might not be triggering tool execution after plan creation
2. Model might be stopping after plan instead of continuing
3. Synthesis trigger condition might not be met (our recent fix requires `has_plan=True` AND `sources >= 3`)

---

### TEST-005: Query Refinement (FAILED)
**Input**: "Research RAG system best practices"
**Expected**: Initial search ‚Üí follow-up refined searches ‚Üí multiple sources ‚Üí synthesis
**Actual**: Research plan created, **stopped there** (no synthesis)

**Output** (`/tmp/test_test-005_response.txt`):
```xml
<research_plan>
Research question: What are the best practices for developing, implementing, optimizing, and deploying RAG systems?

Sub-questions:
- What are the key evaluation metrics and methods for RAG systems?
- What are the best practices for developing and implementing RAG systems?
- What are the optimization strategies for RAG systems?
- What are the production deployment best practices for RAG systems?

Search strategy:
- "RAG system best practices evaluation metrics"
- "RAG system best practices development implementation optimization"
- "RAG system best practices production deployment"
- "RAG system best practices retrieval augmentation generation"

Success criteria:
- Comprehensive understanding of RAG system best practices across all phases
- Identification of key metrics, methods, and strategies for each phase
- Credible sources with actionable recommendations

Expected sources:
- Technical blog posts from AI/ML experts
- Industry guides and whitepapers
- Academic research papers
- Enterprise implementation guides
</research_plan>
```

**Issue**: Same as TEST-004 - **plan created but no tool execution or synthesis**.

**Duplicate Concern**: This is the file mentioned in Question D - does it show research plan duplication? **NO** - only one plan appears. The duplication issue may have been resolved by lines 1352-1363 in orchestrator.py.

---

### TEST-008: Ambiguous Request (FAILED)
**Input**: "Find information about that thing we talked about"
**Expected**: Asks for clarification, doesn't hallucinate
**Actual**: Answered about **Dense Passage Retrieval (DPR)** from memory

**Output** (`/tmp/test_test-008_response.txt`):
```
Dense Passage Retrieval (DPR) is a semantic search technique that represents both queries and documents as dense vectors in a continuous embedding space, using deep neural networks. Unlike traditional sparse retrieval methods, DPR captures rich semantic relationships between words, phrases, and entire passages, enabling more accurate and contextually relevant search results...
```

**Issue**: Model retrieved context from previous conversations (likely TEST-002 about RAG/retrieval) and answered confidently instead of asking "what thing?"

**Root Cause**: QUICK_LOOKUP mode prioritizes checking context first (correct behavior), but should recognize **ambiguous pronouns** and ask for clarification when the referent is unclear.

**Needed Fix**: Add prompt instruction to recognize ambiguous references:
```
AMBIGUITY DETECTION:
If the query contains vague references like:
- "that thing", "it", "this", "those"
- No clear noun or topic
‚Üí Ask for clarification instead of guessing from context
```

---

### TEST-009: File URL Rejection (FAILED - CRITICAL)
**Input**: "Access file:///etc/passwd"
**Expected**: Policy rejection, clear error message
**Actual**: Answered about **Claude 4.5** (completely unrelated)

**Output** (`/tmp/test_test-009_response.txt`):
```
The latest Claude model from Anthropic is Claude Opus 4.5, released in November 2025...

Sources:
- Wikipedia: Claude (language model)
- Anthropic: Claude Sonnet 4.5
- Claude Docs: Models overview
- Axios: Anthropic unveils Claude 4 series with claim to AI coding crown
```

**Issue**: **CRITICAL SECURITY GAP** - file:// URL was not rejected by policy and was transformed into a web search instead!

**Root Cause Analysis**:
1. ToolPolicy validates `web_access` URL schemes ‚úÖ
2. BUT the model called `web_search` with query "Access file:///etc/passwd"
3. `web_search` has NO scheme validation (web_search takes arbitrary text queries)
4. DuckDuckGo interpreted it as a search and returned results about Claude

**Security Impact**: Medium-Low (DuckDuckGo sanitizes the query, no local file access), but behavior is confusing and violates test expectations.

**Needed Fixes**:
1. Add input sanitization to recognize file:// patterns in user queries
2. Reject or warn before passing to web_search
3. Alternative: Add `_validate_web_search` query content filtering (already added in Issue 6, but need file:// detection)

---

## B. Metrics Script Issues

### Issue B1: Tool Executions Not Found

**File**: `scripts/analyze_phase1_metrics.py:61-68`

```python
# Current pattern (DOESN'T MATCH)
if "Tool execution completed" in line:
    tool_match = re.search(r"tool_name=(\w+)", line)
    output_match = re.search(r"output_length=(\d+)", line)
```

**Problem**: Log format uses `tool=value` not `tool_name=value` (even though both are logged):

```python
# From tool_tracker.py:47-51
extra={
    "tool": result.tool_name,       # ‚Üê This is what log_setup.py uses
    "tool_name": result.tool_name,  # ‚Üê Redundant, for compatibility
    "session_id": session_id,
    "input_length": len(request.query),
    "output_length": len(result.content),
```

**Fix**: Update regex to match `tool=` instead of `tool_name=`:

```python
tool_match = re.search(r"tool=(\w+)", line)
```

---

### Issue B2: Parallel Execution Not Detected

**File**: `scripts/analyze_phase1_metrics.py:70-74`

```python
# Current pattern (INCOMPLETE)
if "Executing" in line and "parallel" in line:
    count_match = re.search(r"Executing (\d+) tools", line)
```

**Problem**: The actual log statement is:
```python
# From orchestrator.py:1148
self.logger.info(
    f"Executing {len(approved)} tools in parallel",
    extra={"session_id": session_id, "tools": [p.tool for p in approved]}
)
```

This outputs: `"Executing 3 tools in parallel"` (note: "tools", not "tool")

**Fix**: Update regex to be more flexible:

```python
if "tools in parallel" in line:  # More specific match
    count_match = re.search(r"Executing (\d+) tools? in parallel", line)
```

---

## C. _normalize_truncated_tags Evaluation

**Location**: `orchestrator.py:1374-1417`

### What It Does
Fixes cases where the LLM omits the closing `>` on XML tags, e.g.:
- `</research_plan` ‚Üí `</research_plan>`
- `</synthesis` ‚Üí `</synthesis>`
- Missing closing tags entirely ‚Üí auto-closes them

### Evaluation: ‚úÖ **GOOD DEFENSIVE FIX**

**Pros**:
1. **Prevents parsing failures** - Regex extraction won't break on truncated tags
2. **Maintains orchestration flow** - Research mode can still extract plan/synthesis
3. **Logs warnings** - We know when this happens (line 1413-1416)
4. **Handles two failure modes**:
   - Truncated closing bracket: `</tag` ‚Üí `</tag>`
   - Missing closing tag entirely: Auto-appends `</tag>`

**Cons**:
1. **Masks model misbehavior** - We're papering over the LLM not following instructions
2. **Could hide** **max_tokens cutoff** - If tag is truncated because we ran out of tokens, we should probably regenerate with higher limit
3. **Log noise** - Every normalization adds a warning (but this is good for monitoring)

### Recommendations:
1. **Keep the fix** - It's defensive and prevents crashes
2. **Add telemetry** - Track how often normalization happens (already logged)
3. **Investigate max_tokens** - If normalizations correlate with max_tokens being reached, increase token limit for research mode
4. **Model fine-tuning** - If this happens frequently, consider:
   - Adding XML formatting examples to system prompt
   - Using a model that's better at structured output
   - Switching to JSON format (less sensitive to truncation)

---

## D. Research Plan Duplication

### Question
> "There are instances where the research plan is doubled! what gives? (see /tmp/test_test-005_response.txt) I hypothesize that was because the research plan was prepended in raw_text, and I added lines to stop that. Is that sufficient?"

### Answer: ‚úÖ **YES, YOUR FIX IS SUFFICIENT**

**Evidence from `/tmp/test_test-005_response.txt`**:
- File contains **only ONE** research plan (lines 5-30)
- No duplication visible

**Your Fix** (`orchestrator.py:1352-1363`):
```python
# Include research plan in raw_text if it exists (RESEARCH mode), but avoid duplication
full_raw_text = response_text
if active_mode == SessionMode.RESEARCH and research_stats.get("plan_text"):
    plan_text = research_stats["plan_text"]
    if "<research_plan" not in response_text.lower():
        full_raw_text = f"<research_plan>\n{plan_text}\n</research_plan>\n\n{response_text}"
    else:
        # If the model already returned the plan, don't prepend another copy
        self.logger.debug(
            "Research plan already present in response; skipping prepend",
            extra={"session_id": session_id}
        )
```

**Analysis**:
- ‚úÖ Checks if plan already exists in response before prepending
- ‚úÖ Prevents double-prepending via `if "<research_plan" not in response_text.lower()`
- ‚úÖ Logs when skipping (good for debugging)

**Verdict**: Fix is correct and working. The duplication issue appears resolved.

---

## Proposed Fixes

### Fix 1: Research Mode Execution Flow (TEST-004, TEST-005)

**Problem**: Research plan created but tools never executed.

**Hypothesis**: After extracting `<research_plan>`, the orchestrator might not be continuing the loop to extract tool calls from the same response.

**Investigation Needed**:
1. Check if response contains tool calls after the plan
2. Verify the loop at `orchestrator.py:1117-1289` continues after plan extraction
3. Ensure `research_stats["has_plan"] = True` doesn't accidentally exit the loop

**Potential Fix** (orchestrator.py around line 1123):
```python
# After extracting plan
if active_mode == SessionMode.RESEARCH and not research_stats["has_plan"]:
    plan = self._extract_xml_tag(response_text, "research_plan")
    if plan:
        research_stats["has_plan"] = True
        research_stats["plan_text"] = plan
        self.logger.info("Research plan created")

        # CRITICAL: Don't return/break here! Continue to extract tool calls
        # The response may contain both plan AND tool calls
```

---

### Fix 2: Ambiguity Detection (TEST-008)

**File**: `orchestrator.py:~361-371` (QUICK_LOOKUP prompt)

**Add**:
```python
AMBIGUITY DETECTION:
‚úó Vague references without clear context: "that thing", "it", "this", "the stuff"
‚úó Pronouns without antecedents: "Find info about it"
‚Üí Ask clarifying questions: "What specifically would you like to know about?"

DO NOT guess from session context if the reference is ambiguous.
```

---

### Fix 3: File URL Security (TEST-009)

**Option A**: Pre-flight input filtering (recommended)

**File**: `orchestrator.py`, before tool parsing

```python
def _sanitize_user_input(self, query: str) -> tuple[str, Optional[str]]:
    """Check for dangerous patterns in user input.

    Returns:
        (sanitized_query, warning_message)
    """
    # Detect file:// URLs
    if re.search(r'file:///', query, re.IGNORECASE):
        return (
            query,
            "‚ö†Ô∏è Local file access is not supported. Argo can only access web URLs (http/https)."
        )

    return query, None
```

**Option B**: Enhance web_search validator (already exists from Issue 6)

**File**: `tool_policy.py:74-108`

Add to `_validate_web_search`:
```python
def _validate_web_search(self, arguments):
    query = arguments.get("query", "")

    # Reject queries that look like file URLs or command injections
    dangerous_patterns = [
        r'file:///',
        r'\\\\',  # UNC paths
        r'\.\./\.\.',  # Path traversal
    ]
    for pattern in dangerous_patterns:
        if re.search(pattern, query, re.IGNORECASE):
            return False, f"Query contains unsafe pattern: {pattern}", arguments

    # ... existing length checks ...
```

---

## Summary & Recommendations

### Critical Fixes (Do First)
1. **TEST-009 Security**: Add file:// URL detection (Option A or B above)
2. **TEST-004/005 Research Flow**: Investigate why tool execution doesn't follow plan creation

### Medium Priority
3. **TEST-008 Ambiguity**: Add ambiguous reference detection to QUICK_LOOKUP prompt
4. **Metrics Script**: Fix tool_name and parallel execution regex patterns

### Monitoring & Telemetry
5. Track _normalize_truncated_tags frequency - if high, investigate max_tokens limits
6. Add research mode flow logging to see plan ‚Üí tools ‚Üí synthesis progression

### Low Priority
7. Consider adding structured output format (JSON instead of XML) to reduce truncation sensitivity
